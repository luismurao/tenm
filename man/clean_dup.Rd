% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clean_dup.R
\name{clean_dup}
\alias{clean_dup}
\title{Function to clean duplicated longitude and latitude data}
\usage{
clean_dup(
  data,
  longitude,
  latitude,
  threshold = 0,
  by_mask = FALSE,
  raster_mask = NULL,
  n_ngbs = 0
)
}
\arguments{
\item{data}{A data.frame with longitude and latitude of occurrence records
belongings to some specie.}

\item{longitude}{A character vector of the column name "longitude" within
the dataframe.}

\item{latitude}{A character vector of the column name of "latitude" within
the dataframe.}

\item{threshold}{A numeric value representing the euclidean distance between
coordinates to be considered as a duplicate. Also it could be view as a
value of radio (r) that covers an area.}

\item{by_mask}{Logical. If TRUE the elimination of duplicates will be done
using a raster layer as a mask; If False the elimination of duplicates will
be done by the distance threshold.}

\item{raster_mask}{An object of class SpatRaster that will be used to clean
duplicates that are present in the same ID pixel.}

\item{n_ngbs}{Number of pixel neighbors. Remove duplicates depending on how
many pixels range you want, 1 is for eliminate duplicates in the same pixel,
that means just one record per single pixel of resolution,2 is a neighborhood
with one-pixel length of 3 for 3 pixels, 3 is an 5 for 5 vicinity and so on
depending on how much area you want to cover.}
}
\value{
Returns a data.frame with coordinate data from a species
}
\description{
Clean up duplicated or redundant occurrence records that
present overlapping longitude and latitude geographical coordinates regarding
a referent system that the user can choose from fourth possible ways to
eliminate: distance threshold, per single pixel grain of resolution, by pixel
neighborhood, or both combined distance and pixel at the same time.This
function has as its main purpose to eliminate occurrence points
geographically spliced, using the information of the spatial position to
determine it, in order to debug large clouds of occurrence points that
could cause an environmental overestimation in the future model, we call this
process "cleaning of spatial duplicated data". The user has to set a distance
in "units" that arose an overlapping area from a random chosen record where
any other occurrence within this is consider as a duplicated and then
eliminated. Also you can use a raster mask with a determinate pixel grain
resolution as a base sift; see in arguments raster_mask and n_ngbs.
}
\examples{
data(abronia)
tempora_layers_dir <- system.file("extdata/bio",package = "tenm")
tenm_mask <- terra::rast(file.path(tempora_layers_dir,"1939/bio_01.tif"))
# Clean duplicates without raster mask (just by distance threshold)
# First check the number of occurrence records
print(nrow(abronia))
# Clean duplicated records using a distance of ~ 18 km (0.1666667 grades)
ab_1 <- tenm::clean_dup(data =abronia,
                        longitude = "decimalLongitude",
                        latitude = "decimalLatitude",
                        threshold = terra::res(tenm_mask),
                        by_mask = FALSE,
                        raster_mask = NULL)
# Check number of records
print(nrow(ab_1))
# Clean duplicates using a raster mask
ab_2 <- tenm::clean_dup(data =abronia,
                        longitude = "decimalLongitude",
                        latitude = "decimalLatitude",
                        threshold = terra::res(tenm_mask)[1],
                        by_mask = TRUE,
                        raster_mask = tenm_mask,
                        n_ngbs = 1)
# Check number of records
print(nrow(ab_2))
}
